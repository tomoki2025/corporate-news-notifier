const fs = require("fs");
const path = require("path");
const puppeteer = require("puppeteer");

const url = "https://spacedata.jp/news";
const dataFilePath = path.join(__dirname, "data", "spacedata.json");

(async () => {
  const browser = await puppeteer.launch({
    headless: "new",
    args: ["--no-sandbox", "--disable-setuid-sandbox"]
  });
  const page = await browser.newPage();
  await page.goto(url, { waitUntil: "networkidle2" });

  // ðŸ”½ è¿½åŠ ï¼šã‚»ãƒ¬ã‚¯ã‚¿ã‚’æ˜Žç¤ºçš„ã«å¾…ã¤ï¼ˆæœ€å¤§10ç§’ï¼‰
  try {
    await page.waitForSelector(".news_list li a", { timeout: 10000 });
  } catch (e) {
    console.error("âŒ .news_list li a ãŒ10ç§’å¾…ã£ã¦ã‚‚å‡ºç¾ã—ã¾ã›ã‚“ã§ã—ãŸ");
    await browser.close();
    return;
  }

  const anchors = await page.$$eval(".news_list li a", (elements) => {
    return elements.map((a) => ({
      title: a.querySelector("p")?.innerText.trim(),
      url: a.href,
      date: a.querySelector("time")?.innerText.trim()
    }));
  });

  console.log("âœ… anchor length:", anchors.length);
  console.log("âœ… first anchor sample:", anchors[0]);

  await browser.close();

  let oldArticles = [];
  if (fs.existsSync(dataFilePath)) {
    const rawData = fs.readFileSync(dataFilePath);
    oldArticles = JSON.parse(rawData);
  }

  const oldUrls = new Set(oldArticles.map((a) => a.url));
  const newArticles = anchors.filter((a) => !oldUrls.has(a.url));

  if (newArticles.length > 0) {
    const allArticles = [...newArticles, ...oldArticles];
    fs.writeFileSync(dataFilePath, JSON.stringify(allArticles, null, 2));
  }

  console.log(`ðŸŸ¡ Scraping completed. New articles: ${newArticles.length}`);
})();
